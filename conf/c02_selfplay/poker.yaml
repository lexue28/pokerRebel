defaults:
  - optimizer: adam

seed: 0

# Device: "cuda" for GPU training, "cpu" for CPU-only
# If PyTorch doesn't have CUDA support, set to "cpu"
device: cuda

# Learning rate schedule
decrease_lr_every: 400
decrease_lr_times: 2
grad_clip: 5.0

env:
  # Poker variant identifier (used by env factory)
  game: toss_holdem

  # Exploration during SAMPLELEAF
  random_action_prob: 0.25
  sample_leaf: true

  subgame_params:
    # CFR iterations per subgame
    # ULTRA-SAFE defaults for CPU (prevents OOM during warmup)
    # Scale up after first successful checkpoint: 16→32→64 for num_iters, 2→3→4 for max_depth
    num_iters: 16         # Ultra-safe for first run (was 256 - causes OOM)
    max_depth: 2          # Ultra-safe for first run (was 4 - causes OOM)
    linear_update: true

exploit: true

selfplay:
  network_sync_epochs: 1
  dump_dataset_every_epochs: 200

  # GPU settings (disabled)
  threads_per_gpu: -1
  models_per_gpu: 1

  # CPU-only data generation
  cpu_gen_threads: 1      # Ultra-safe for first run (was 8 - reduces parallel memory blowup)

train_gen_ratio: 4
task: selfplay

loss: huber

min_buffer_to_send: 2500
max_epochs: 3000          # Smaller than LD

model:
  name: Net2
  kwargs:
    n_hidden: 256
    n_layers: 2
    use_layer_norm: true

create_validation_set_every: 100

data:
  train_epoch_size: 200   # Ultra-safe for first run (was 12800)
  train_batch_size: 16    # Ultra-safe for first run (was 512)

replay:
  capacity: 1000          # Ultra-safe for first run (was 500000 - prevents OOM)
  alpha: 1.0
  beta: 1.0
  prefetch: 8
  use_priority: false
